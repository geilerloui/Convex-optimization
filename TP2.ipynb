{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque à étudier:\n",
    "* il montre que le dual du least norm est concave (alors que c'est des produit de matrice) comprend pas\n",
    "* The vectors $\\lambda$ and $\\nu$ are called the dual variables or Lagrange multiplier vectors.\n",
    "\n",
    "#### Dual optimization (page 117)\n",
    "\n",
    "notation: lambda pour les contraintes inférieur ou égale; et nu pour les contraintes égale.\n",
    "\n",
    "\n",
    "#### Slide 5-2:\n",
    "\n",
    "$h_i(x)$ c'est un résidus, donc si à zéro, c'est parfait.\n",
    "\n",
    "#### Slide 5-3:\n",
    "\n",
    "On remarque que le lagrangian est une fonction de $x, \\lambda, \\nu$ alors que\n",
    "\n",
    "le **Lagrange Dual Function** est une fonction uniquement de $\\lambda, \\nu$, une interprétation possible est que c'est le coût optimal sachant les prix $\\lambda_i$ et $\\nu_i$. Remarque: la fonction $g(\\lambda, \\nu)$ est concave même si le problème initial est non convexe. Pourquoi ? déjà on peut voir que en fonction de $\\lambda$ et $\\nu$ on a une fonction affine. Or le inf sur une fonction affine est une fonction concave.\n",
    "\n",
    "La lower bound property nous dit que le dual est un lower bound sur le primal\n",
    "\n",
    "#### Slide 5-6:\n",
    "\n",
    "Un exemple de problème non convexe. -> peut être à re regarder car y'a masse truc\n",
    "\n",
    "\n",
    "#### Slide 5-7:\n",
    "\n",
    "Il y'a une forte connexion entre le lagrange dual et la fonction conjugué.\n",
    "On rappelle la définition du conjugué d'une fonction f:\n",
    "\n",
    "$$\n",
    "f^*(y) = \\underset{x \\in dom~f}{sup} (y^T x - f(x))\n",
    "$$\n",
    "\n",
    "On note que $f^*$ est convexe même si $f$ ne l'est pas.\n",
    "\n",
    "#### Slide 5-8:\n",
    "\n",
    "C'est le Dual Problem.\n",
    "\n",
    "#### Slide 5-9:\n",
    "\n",
    "On note $p^*$ la valeur optimale du lagrangien et $d^*$ la valeur optimale du problème d'optimisation.\n",
    "\n",
    "#### Slide 5-10:\n",
    "\n",
    "weak duality $d^* \\le p^*$;\n",
    "\n",
    "\n",
    "strong duality $d^* = p^*$ --> **constraint qualifications** conditions pour une strong duality + que le problème soit convexe\n",
    "\n",
    "**duality gap**: $p^* - d^*$ \n",
    "\n",
    "#### Slide 5-11:\n",
    "\n",
    "condition de slater dit juste que la contrainte doit être strictement inférieur et si le programme est convexe on est OK, on a bien un duality gap de 0\n",
    "\n",
    "Le strong slater condition dit que $Ax \\le b$\n",
    "\n",
    "--> J'ai l'impression que la condition de slater tient pour certains $x$ comme c'est écris\n",
    "\n",
    "#### Slide 5-12:\n",
    "\n",
    "Slides d'après c'est des exemples de différents problèmes\n",
    "\n",
    "#### Slide 5-14:\n",
    "\n",
    "Exemple de problème non convexe\n",
    "\n",
    "\\begin{array}{ll}\\mbox{maximize} & X^T P X\\\\ \\mbox{subject to} & ||x||_2 = 1  \\end{array}\n",
    "\n",
    "On prend le plus grand eigenvalue de P et x est le eigenvector associé avec ça.   \n",
    "Mais ce problème n'est pas covnexe, car P peut être n'importe quoi, et la contrainte est non convexe, ou alors il faudrait la relaxer en $||x||_2 \\le  1$.\n",
    "\n",
    "C'est la base de PCA, SVD ... MAIS, on peut les résoudre en utilisant l'optimisation convexe, alors que ce sont des problèmes non convexe\n",
    "\n",
    "Il dit qu'il est mieux d'écrire le problème comme un SDP (plus propre)\n",
    "\n",
    "\n",
    "#### Slide 5-15:\n",
    "\n",
    "Le X axis $u$ sera la fonction contrainte $f_1$; Le Y axis sera $t$ est la fonction contrainte $f_0$\n",
    "\n",
    "Comme préciser dans l'énnoncé, on va considérer uniquement la contrainte $f_1(x) \\le 0$, on va donc regarder uniquement les points réalisables à gauche.\n",
    "\n",
    "Puis pour $f_0(x)$ on va le minimiser, et on aura $p^*$ (qu'on lira sur l'axe des ordonnées, c'est un scalaire pas un vecteur)\n",
    "\n",
    "Pour trouver le dual, on va calculer le produit scalaire de: $(\\lambda, 1) \\dot (f_1(x), f_0(x))=f_0(x) + \\lambda f_1(x)$, puis, si j'ai bien compris c'est l'équation d'une droite on va pouvoir en tracer, jusqu'à ce qu'on touche les deux intersections de G et on aura $d^*$ comme scalaire sur l'axe $t$ (\"the best line that is tangent to the set G\")\n",
    "\n",
    "on va remplacer l'ensemble G par son epigraphe A, on dira que l'ensemble est convexe si $f_1(x),f_0(x)$ sont convexe.\n",
    "\n",
    "--> Revoir l'histoire des slaters conditions avec l'epigraph\n",
    "\n",
    "\n",
    "#### Slide 5-17:\n",
    "\n",
    "Complementary slackness:\n",
    "\n",
    "On suppose que $f_0(x^*)$ est primal réalisable et que $g(\\lambda^*, \\nu^*)$ est dual réalisable.\n",
    "\n",
    "dans la seconde ligne, en prenant un point qui est primal optimal on aura forcément une valeur inférieur ou égale au lagrangien. On aura égalité uniquement si cette équation minimise le lagrangien\n",
    "\n",
    "Puis on aura que $\\sum \\lambda_i^* f_i(x^*)$ est inférieur ou égale à 0, le terme d'après égale à zéro (on rappelle les contraintes doivent être inf ou égale et égale pour la dernière). \n",
    "\n",
    "Enfin, suite à notre raisonnement, on voit qu'on a égalité entre tous les termes\n",
    "\n",
    "Quand on parle de \"complementary slackness\" ça signifie que soit $\\lambda_i \\ge 0$ ou que $f_i(x) \\le 0$ si un des deux est serrés alors les deux sont serrés. Mais ce n'est pas posssible que les deux soient serrés en même temps.\n",
    "\n",
    "#### Slide 5-18:\n",
    "\n",
    "\n",
    "On rappelle le completementary slackness, si $\\lambda_i$ est positive alors $f(x)$ doit être égale à zéro; ou inversement (si $f_i$ est négative alors $\\lambda_i =0 $\n",
    "\n",
    "la condition 4 s'écrit:\n",
    "\n",
    "$$\n",
    "\\nabla_x L = \\nabla f_0(x) + \\sum_{i=1}^{n} \\lambda_i \\nabla f_i(x) + \\sum_{i=1}^{p} \\nu_i \\nabla h_i(x)=0\n",
    "$$\n",
    "\n",
    "\n",
    "#### Slide 5-18:\n",
    "\n",
    "On peut dire que le KTT est les conditions nécessaires et suffisantes pour l'optimalité d'un problème d'optimisation convexe avec des contraintes d'égalités et inégalités. Mais qui suppose la différentiabilité des variables\n",
    "\n",
    "\n",
    "#### Slide 5-20:\n",
    "\n",
    "on a trois valeurs: $\\lambda \\ge 0$, $\\lambda_i x_i = 0$ (complementary slackness) et l'expression d'après c'est la dérivé.\n",
    "\n",
    "Après on peut faire les calcul si dans  $\\lambda_i x_i = 0$, $x_i=0$ alors dans l'expression de la dérivé on peut trouver ce qu'est $\\lambda_i = \\nu - 1 / \\alpha_i$\n",
    "\n",
    "Le second cas est que $x=0$\n",
    "\n",
    "Puis on va s'occuper de $1^T x = 1$, on voit qu'on fait le max des deux cas possibles pour $x$\n",
    "\n",
    "Et on peut remarquer les deux situations possibles, avec $\\nu$ grand ou petit.\n",
    "\n",
    "**Water filling** algorithme de théorie de l'information\n",
    "  \n",
    "#### Slide 5-21:\n",
    "\n",
    "On commence avec un programme qui a que des variables inférieur ou égale à zéro. On donne son dual qui est basiquement le lagrangian.\n",
    "\n",
    "On définit un problème perturbé, qui est d'enlever les zéros et les remplacer par $u_i$, ce qui va changer le dual.\n",
    "\n",
    "Interprétation:\n",
    "* $v_i$: plus dure à interpréter\n",
    "* $u_i$: disons qu'elle est égale à $0.1$; il faut voir ça en disant que $f_i(x)$ de base doit être inférieur ou égale à zéro. On peut dire que c'est une relaxation car le feasible set est plus grand. Mais peut être importe ce qu'il se passe $p^*(u,v)$ diminuera. On dira que avec une valeur positive de $u_i$ on a loosen l'inégalité si elle est négative on a tighten\n",
    "\n",
    "Note:\n",
    "* $p^*(u,v)$ si on tighten trop la valeur peut être $+ \\infty$ car il n'y a pas de valeurs possibles.\n",
    "* $p^*$ nous dit comment la valeur optimale change quand on serre ou déserre la contraintes d'inégalités et on peut interpréter $v$ comme un déplacement.\n",
    "\n",
    "#### Slide 5-22:\n",
    "\n",
    "$p^*(u,v) \\ge p^*(0,0) - u^T\\lambda^* - v^T \\mathcal{v}$  \n",
    "Cette partie $p^*(0,0)$ est le programme non perturbé. Cette équation pose un lower bound sur le résultat\n",
    "\n",
    "--> Sensitivity interpretation: plutôt simple à comprendre.\n",
    "\n",
    "\n",
    "Local sensitivity:\n",
    "\n",
    "on suppose $p^*(u, v)$ \n",
    "\n",
    "Il donne un exemple en circuit design: imaginons que nos contraintes soient tight, mais qu'on a les valeurs suivantes, $\\lambda_1^*=0.1$ (pour le power) et $\\lambda_2^*=100$ pour l'aire du circuit. Ça signifie que la contrainte sur le power est moins serrer, si on baisse la valeur on aura la même fonction objectif.\n",
    "\n",
    "Les multiplicateurs de lagranges donnent une très bonne mesure quantitative, si la valeur est proche de zéro ça signifie qu'elle n'est pas trop tight, sinon (genre 100) qu'elle est très tight et qu'on ne pourra pa la bouger du tout.\n",
    "\n",
    "Peut être à re regarder à partir de 16min,\n",
    "\n",
    "complementary slackness\n",
    "inequality constraintes peuvent être tight ou slack\n",
    "\n",
    "#### Slide 5-24:\n",
    "\n",
    "On peut tracer un commutative diagram\n",
    "\n",
    "$D \\iff P \\iff P' \\iff D'$\n",
    "\n",
    "On peut reformuler le primal initial par P' un ^problème équivalent.  \n",
    "Mais l'affirmation $D \\iff D'$ est impossible, mais on verra que c'est une bonne chose, car on verra des situations ou D ou D' sera trivial ou intéressant.\n",
    "\n",
    "#### Slide 5-25:\n",
    "\n",
    "On donne un exemple de reformulation, car le premier problème du primal n'a pas de contrainte, et on se retrouve avec un dual qui n'est pas utile.\n",
    "\n",
    "Le dual de la reformulation peut servir de condition d'arrêt. Si on a que $\\nu$ finit dans le null space (la contrainte) alors on peut calculer la valeur à maximiser.\n",
    "\n",
    "Il peut arriver que les gens fassent un peu de transformation avant de former le dual, ou après l'avoir transformé.\n",
    "\n",
    "On peut  noter que quand les gens attachent un nom comme le \"frank wolfe dual\".. ça signifie qu'il y'a eu quelques arrangements.\n",
    "\n",
    "#### Slide 5-26:\n",
    "\n",
    "Quand on le infimum d'une fonction affine et d'un terme linéaire, on peut le réecrire comme le conjugué\n",
    "\n",
    "#### Slide 5-27:\n",
    "\n",
    "Appelé aussi : partial lagrangian ou partial dualization\n",
    "\n",
    "-> pas terminé le dernier algo la flemme\n",
    "\n",
    "#### Slide 5-28:\n",
    "\n",
    "Problème différents, dans les contraintes on pourra avoir des vecteurs. Les contraintes devront être respecté par rapport à un cône. Le cas non trivial est le semi definite cone.\n",
    "\n",
    "#### Slide 5-29:\n",
    "\n",
    "ils sont non négatif dans le dual cone.\n",
    "\n",
    "La preuve est similaire, la différence eset l'ajout de vecteurs pour les paramètres\n",
    "\n",
    "#### Slide 5-30:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Slide finale:\n",
    "\n",
    "\n",
    "Read chapter 5 (omit 5.2.5, 5.5.4, skim 5.4, 5.8, 5.9.4) of the book.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Duality\n",
    "\n",
    "#### 3.1 Numerical perturbation analysis example\n",
    "\n",
    "Consider the quadratic program\n",
    "\n",
    "\\begin{array}{ll}\\mbox{minimize} & x_1^2 + 2x_2^2 -x_1 x_2 - x_1\\\\ \\mbox{subject to} & x_1 + 2 x_2 \\leq u_1 \\\\ & x_1 - 4x_2 \\leq u_2, \\\\& x_1+ x_2 \\geq -5, \\end{array}\n",
    "\n",
    "with variables $x_1$, $x_2$, and parameters $u_1$, $u_2$.\n",
    "\n",
    "(a) Solve this QP, for parameter values $u_1=-2$, $u_2=-3$, to find optimal primal variable values $x^\\star_1$ and $x_2^\\star$, and optimal dual variable values $\\lambda_1^\\star$, $\\lambda_2^\\star$ and $\\lambda_3^\\star$. Let $p^\\star$ denote the optimal objective value. Verify that the KKT conditions hold for the optimal primal and dual variables you found (within reasonable numerical accuracy).\n",
    "\n",
    "We recall that the quadratic form of a product of matrices is:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "x_1 & x_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "a & b \\\\\n",
    "c & d \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "x_1  \\\\\n",
    "x_2 \n",
    "\\end{bmatrix}\n",
    "= ax_1^2 + (c+b) \\times x_1x_2+dx_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: See $\\S$4.7 of the CVX users' guide to find out how to retrieve optimal dual variables. To specify the quadratic objective, use quad_form().\n",
    "\n",
    "What is $x_2^\\star$? Enter your result rounded to two decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.16666667;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is $\\lambda_3^\\star$? Enter your result rounded to two decimal places.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.60686772272851e-09;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal (Ax <= b) dual variable [3.38882023 2.44439742]\n",
      "optimal (x1 + x2 >= -5) dual variable 6.60686772272851e-09\n",
      "status: optimal\n",
      "optimal value 8.222222115904906\n",
      "optimal var [-2.33333333  0.16666667]\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = cp.Variable(2)\n",
    "Q = np.array([[1, -1/2], [-1/2, 2]])\n",
    "f = np.array([-1, 0])\n",
    "A = np.array([[1, 2], [1, -4]])\n",
    "b = np.array([-2, -3])\n",
    "\n",
    "# Create four constraints\n",
    "constraints = [\n",
    "    A*x <= b,\n",
    "    np.transpose(np.array([1, 1]))*x >= -5\n",
    "]\n",
    "\n",
    "obj = cp.Minimize(cp.quad_form(x, Q) + np.transpose(f)*x)\n",
    "\n",
    "# Form and solve problem.\n",
    "prob = cp.Problem(obj, constraints)\n",
    "prob.solve()\n",
    "\n",
    "# The optimal dual variable (Lagrange multiplier) for\n",
    "# a constraint is stored in constraint.dual_value.\n",
    "print(\"optimal (Ax <= b) dual variable\", constraints[0].dual_value)\n",
    "print(\"optimal (x1 + x2 >= -5) dual variable\", constraints[1].dual_value)\n",
    "# Optimal values\n",
    "print(\"status:\", prob.status)\n",
    "print(\"optimal value\", prob.value)\n",
    "# Vector of values generated\n",
    "print(\"optimal var\", x.value[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) We will now solve some perturbed versions of the QP, with\n",
    "\n",
    "$ u_1 = -2 + \\delta_1, \\qquad u_2 = -3 + \\delta_2,$\n",
    "\n",
    "where $\\delta_1$ and $\\delta_2$ each take values from $\\{-0.1, 0, 0.1\\}$. (There are a total of nine such combinations, including the original problem with $\\delta_1=\\delta_2=0$.) For each combination of $\\delta_1$ and $\\delta_2$, make a prediction $p^\\star_\\mathrm{pred}$ of the optimal value of the perturbed QP, and compare it to $p^\\star_\\mathrm{exact}$, the exact optimal value of the perturbed QP (obtained by solving the perturbed QP). Find the values that belong in the two righthand columns in a table with the form shown below. Check that the inequality $p^\\star_\\mathrm{pred} \\leq p^\\star_\\mathrm{exact}$ holds.\n",
    "\n",
    "\\begin{array}{|r||r|r|r|r|}\\hline\\mathrm{\\#}&\\delta_1 & \\delta_2 & p^\\star_\\mathrm{pred} & p^\\star_\\mathrm{exact}\\\\\\hline1&0 & 0 & & \\\\2&0 & -0.1 & & \\\\3&0 & 0.1 & & \\\\\\hline4&-0.1 & 0 & & \\\\5&-0.1 & -0.1 & & \\\\6&-0.1 & 0.1 & & \\\\\\hline7&0.1 & 0 & & \\\\8&0.1 & -0.1 & & \\\\9&0.1 & 0.1 & & \\\\\\hline\\end{array}\n",
    "\n",
    "For which perturbations (other than number 1) is $p^\\star_\\mathrm{exact}-p^\\star_\\mathrm{pred}$ the smallest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) We will now solve some perturbed versions of the QP, with:\n",
    "$$\n",
    "u_1 = -2 + \\delta_1, ~~~~~u_2 = -3 + \\delta_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta1 = [0, 0, 0, -0.1, -0.1, -0.1, 0.1, 0.1, 0.1]\n",
    "delta2 = [0, -0.1, 0.1, 0, -0.1, 0.1, 0, -0.1, 0.1] \n",
    "\n",
    "df = pd.DataFrame({\"delta1\": delta1, \"delta2\": delta2,\n",
    "                   \"p*pred\": [0]*len(delta1), \"p*exact\": [prob.value]*len(delta1),\n",
    "                   \"distance\": [0]*len(delta1)})\n",
    "df_temp = df.copy()\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    b = np.array([-2 + row['delta1'], -3 + row['delta2']])\n",
    "\n",
    "    # Create four constraints\n",
    "    constraints = [\n",
    "        A*x <= b ,\n",
    "        np.transpose(np.array([1, 1]))*x >= -5\n",
    "    ]\n",
    "\n",
    "    obj = cp.Minimize(cp.quad_form(x, Q) + np.transpose(f)*x)\n",
    "\n",
    "    # Form and solve problem.\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    prob.solve()\n",
    "    \n",
    "    df_temp.loc[idx, \"p*pred\"] = prob.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp[\"distance\"] = df_temp[\"p*exact\"] - df_temp[\"p*pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>p*pred</th>\n",
       "      <th>p*exact</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>8.468889</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>-0.246667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.980000</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>0.242222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.565000</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>-0.342778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>8.815556</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>-0.593333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>8.318889</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>-0.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.887222</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>0.335000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>8.130000</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>0.092222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.648889</td>\n",
       "      <td>8.222222</td>\n",
       "      <td>0.573333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   delta1  delta2    p*pred   p*exact  distance\n",
       "0     0.0     0.0  8.222222  8.222222  0.000000\n",
       "1     0.0    -0.1  8.468889  8.222222 -0.246667\n",
       "2     0.0     0.1  7.980000  8.222222  0.242222\n",
       "3    -0.1     0.0  8.565000  8.222222 -0.342778\n",
       "4    -0.1    -0.1  8.815556  8.222222 -0.593333\n",
       "5    -0.1     0.1  8.318889  8.222222 -0.096667\n",
       "6     0.1     0.0  7.887222  8.222222  0.335000\n",
       "7     0.1    -0.1  8.130000  8.222222  0.092222\n",
       "8     0.1     0.1  7.648889  8.222222  0.573333"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
